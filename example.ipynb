{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git2doc\n",
    "from git2doc import loader\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "turbo_docs.toml:\n",
      "\n",
      "ignore = [\n",
      "    \"static\",\n",
      "    \"venv\",\n",
      "    \"__pycache__\",\n",
      "    \"app.log\",\n",
      "    \".env\",\n",
      "    \".git\",\n",
      "    \"repodata\"\n",
      "]\n",
      "\n",
      "app.py:\n",
      "\n",
      "import json\n",
      "import logging\n",
      "\n",
      "import src.config as config\n",
      "import src.services as services\n",
      "\n",
      "from flask import Flask, render_template\n",
      "from flask_caching import Cache\n",
      "from flask_socketio import SocketIO, send\n",
      "\n",
      "\n",
      "# Set up Flask app and socketio\n",
      "app = Flask(__name__)\n",
      "socketio = SocketIO(app, cors_allowed_origins=\"*\")\n",
      "\n",
      "# Configure caching\n",
      "cache = Cache(app, config={\"CACHE_TYPE\": \"simple\"})\n",
      "\n",
      "# Configure logging\n",
      "handler = logging.FileHandler(\"app.log\")\n",
      "handler.setLevel(logging.INFO)\n",
      "logging.basicConfig(\n",
      "    filename=\"app.log\",\n",
      "    level=logging.DEBUG,\n",
      "    format=\"%(asctime)s - %(levelname)s - %(message)s - %(filename)s:%(lineno)d - %(funcName)s\",\n",
      "    datefmt=\"%d-%b-%y %H:%M:%S\",\n",
      ")\n",
      "app.logger.addHandler(logging.StreamHandler())\n",
      "\n",
      "# Fetch projects from GitHub\n",
      "PROJECTS = services.fetch_projects_info(app)\n",
      "\n",
      "\n",
      "@app.route(\"/\")\n"
     ]
    }
   ],
   "source": [
    "# retrieve a code given a repo URL\n",
    "\n",
    "# using a random repo as an example\n",
    "repo_name = \"https://github.com/voynow/jamievoynow.com\"\n",
    "\n",
    "# return a list of Document objects\n",
    "repo_data = loader.pull_code_from_repo(repo_name)\n",
    "\n",
    "# or return a string of all the raw text\n",
    "raw_repo = loader.docs_to_str(repo_data)\n",
    "\n",
    "print(raw_repo[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'allow_forking': True,\n",
      " 'archive_url': 'https://api.github.com/repos/facebookresearch/audiocraft/{archive_format}{/ref}',\n",
      " 'archived': False,\n",
      " 'assignees_url': 'https://api.github.com/repos/facebookresearch/audiocraft/assignees{/user}',\n",
      " 'blobs_url': 'https://api.github.com/repos/facebookresearch/audiocraft/git/blobs{/sha}',\n",
      " 'branches_url': 'https://api.github.com/repos/facebookresearch/audiocraft/branches{/branch}',\n",
      " 'clone_url': 'https://github.com/facebookresearch/audiocraft.git',\n",
      " 'collaborators_url': 'https://api.github.com/repos/facebookresearch/audiocraft/collaborators{/collaborator}',\n",
      " 'comments_url': 'https://api.github.com/repos/facebookresearch/audiocraft/comments{/number}',\n",
      " 'commits_url': 'https://api.github.com/repos/facebookresearch/audiocraft/commits{/sha}',\n",
      " 'compare_url': 'https://api.github.com/repos/facebookresearch/audiocraft/compare/{base}...{head}',\n",
      " 'contents_url': 'https://api.github.com/repos/facebookresearch/audiocraft/contents/{+path}',\n",
      " 'contributors_url': 'https://api.github.com/repos/facebookresearch/audiocraft/contributors',\n",
      " 'created_at': '2023-06-08T06:41:36Z',\n",
      " 'default_branch': 'main',\n",
      " 'deployments_url': 'https://api.github.com/repos/facebookresearch/audiocraft/deployments',\n",
      " 'description': 'Audiocraft is a library for audio processing and generation '\n",
      "                'with deep learning. It features the state-of-the-art EnCodec '\n",
      "                'audio compressor / tokenizer, along with MusicGen, a simple '\n",
      "                'and controllable music generation LM with textual and melodic '\n",
      "                'conditioning.',\n",
      " 'disabled': False,\n",
      " 'downloads_url': 'https://api.github.com/repos/facebookresearch/audiocraft/downloads',\n",
      " 'events_url': 'https://api.github.com/repos/facebookresearch/audiocraft/events',\n",
      " 'fork': False,\n",
      " 'forks': 613,\n",
      " 'forks_count': 613,\n",
      " 'forks_url': 'https://api.github.com/repos/facebookresearch/audiocraft/forks',\n",
      " 'full_name': 'facebookresearch/audiocraft',\n",
      " 'git_commits_url': 'https://api.github.com/repos/facebookresearch/audiocraft/git/commits{/sha}',\n",
      " 'git_refs_url': 'https://api.github.com/repos/facebookresearch/audiocraft/git/refs{/sha}',\n",
      " 'git_tags_url': 'https://api.github.com/repos/facebookresearch/audiocraft/git/tags{/sha}',\n",
      " 'git_url': 'git://github.com/facebookresearch/audiocraft.git',\n",
      " 'has_discussions': False,\n",
      " 'has_downloads': True,\n",
      " 'has_issues': True,\n",
      " 'has_pages': False,\n",
      " 'has_projects': True,\n",
      " 'has_wiki': False,\n",
      " 'homepage': None,\n",
      " 'hooks_url': 'https://api.github.com/repos/facebookresearch/audiocraft/hooks',\n",
      " 'html_url': 'https://github.com/facebookresearch/audiocraft',\n",
      " 'id': 650945129,\n",
      " 'is_template': False,\n",
      " 'issue_comment_url': 'https://api.github.com/repos/facebookresearch/audiocraft/issues/comments{/number}',\n",
      " 'issue_events_url': 'https://api.github.com/repos/facebookresearch/audiocraft/issues/events{/number}',\n",
      " 'issues_url': 'https://api.github.com/repos/facebookresearch/audiocraft/issues{/number}',\n",
      " 'keys_url': 'https://api.github.com/repos/facebookresearch/audiocraft/keys{/key_id}',\n",
      " 'labels_url': 'https://api.github.com/repos/facebookresearch/audiocraft/labels{/name}',\n",
      " 'language': 'Python',\n",
      " 'languages_url': 'https://api.github.com/repos/facebookresearch/audiocraft/languages',\n",
      " 'license': {'key': 'mit',\n",
      "             'name': 'MIT License',\n",
      "             'node_id': 'MDc6TGljZW5zZTEz',\n",
      "             'spdx_id': 'MIT',\n",
      "             'url': 'https://api.github.com/licenses/mit'},\n",
      " 'merges_url': 'https://api.github.com/repos/facebookresearch/audiocraft/merges',\n",
      " 'milestones_url': 'https://api.github.com/repos/facebookresearch/audiocraft/milestones{/number}',\n",
      " 'mirror_url': None,\n",
      " 'name': 'audiocraft',\n",
      " 'node_id': 'R_kgDOJsyiaQ',\n",
      " 'notifications_url': 'https://api.github.com/repos/facebookresearch/audiocraft/notifications{?since,all,participating}',\n",
      " 'open_issues': 77,\n",
      " 'open_issues_count': 77,\n",
      " 'owner': {'avatar_url': 'https://avatars.githubusercontent.com/u/16943930?v=4',\n",
      "           'events_url': 'https://api.github.com/users/facebookresearch/events{/privacy}',\n",
      "           'followers_url': 'https://api.github.com/users/facebookresearch/followers',\n",
      "           'following_url': 'https://api.github.com/users/facebookresearch/following{/other_user}',\n",
      "           'gists_url': 'https://api.github.com/users/facebookresearch/gists{/gist_id}',\n",
      "           'gravatar_id': '',\n",
      "           'html_url': 'https://github.com/facebookresearch',\n",
      "           'id': 16943930,\n",
      "           'login': 'facebookresearch',\n",
      "           'node_id': 'MDEyOk9yZ2FuaXphdGlvbjE2OTQzOTMw',\n",
      "           'organizations_url': 'https://api.github.com/users/facebookresearch/orgs',\n",
      "           'received_events_url': 'https://api.github.com/users/facebookresearch/received_events',\n",
      "           'repos_url': 'https://api.github.com/users/facebookresearch/repos',\n",
      "           'site_admin': False,\n",
      "           'starred_url': 'https://api.github.com/users/facebookresearch/starred{/owner}{/repo}',\n",
      "           'subscriptions_url': 'https://api.github.com/users/facebookresearch/subscriptions',\n",
      "           'type': 'Organization',\n",
      "           'url': 'https://api.github.com/users/facebookresearch'},\n",
      " 'permissions': {'admin': False,\n",
      "                 'maintain': False,\n",
      "                 'pull': True,\n",
      "                 'push': False,\n",
      "                 'triage': False},\n",
      " 'private': False,\n",
      " 'pulls_url': 'https://api.github.com/repos/facebookresearch/audiocraft/pulls{/number}',\n",
      " 'pushed_at': '2023-06-16T02:19:42Z',\n",
      " 'releases_url': 'https://api.github.com/repos/facebookresearch/audiocraft/releases{/id}',\n",
      " 'score': 1.0,\n",
      " 'size': 652,\n",
      " 'ssh_url': 'git@github.com:facebookresearch/audiocraft.git',\n",
      " 'stargazers_count': 6969,\n",
      " 'stargazers_url': 'https://api.github.com/repos/facebookresearch/audiocraft/stargazers',\n",
      " 'statuses_url': 'https://api.github.com/repos/facebookresearch/audiocraft/statuses/{sha}',\n",
      " 'subscribers_url': 'https://api.github.com/repos/facebookresearch/audiocraft/subscribers',\n",
      " 'subscription_url': 'https://api.github.com/repos/facebookresearch/audiocraft/subscription',\n",
      " 'svn_url': 'https://github.com/facebookresearch/audiocraft',\n",
      " 'tags_url': 'https://api.github.com/repos/facebookresearch/audiocraft/tags',\n",
      " 'teams_url': 'https://api.github.com/repos/facebookresearch/audiocraft/teams',\n",
      " 'topics': [],\n",
      " 'trees_url': 'https://api.github.com/repos/facebookresearch/audiocraft/git/trees{/sha}',\n",
      " 'updated_at': '2023-06-17T20:02:56Z',\n",
      " 'url': 'https://api.github.com/repos/facebookresearch/audiocraft',\n",
      " 'visibility': 'public',\n",
      " 'watchers': 6969,\n",
      " 'watchers_count': 6969,\n",
      " 'web_commit_signoff_required': False}\n"
     ]
    }
   ],
   "source": [
    "# retrieve metadata across many popular repos\n",
    "\n",
    "# GitHub API for top 10 repos in the last 10 days that use Python\n",
    "top_repos = loader.get_top_repos(\n",
    "    n_repos=10, last_n_days=10, language=\"python\", sort=\"stars\", order=\"desc\"\n",
    ")\n",
    "\n",
    "pprint(top_repos['items'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing https://github.com/s0md3v/roop...\n",
      "Processing https://github.com/PromtEngineer/localGPT...\n",
      "Processing https://github.com/facebookresearch/audiocraft...\n",
      "Processing https://github.com/Zeqiang-Lai/DragGAN...\n",
      "Processing https://github.com/ShishirPatil/gorilla...\n",
      "Processing https://github.com/baichuan-inc/baichuan-7B...\n",
      "Processing https://github.com/kyegomez/tree-of-thoughts...\n",
      "Processing https://github.com/JiauZhang/DragGAN...\n",
      "Processing https://github.com/SysCV/sam-hq...\n",
      "Processing https://github.com/facebookresearch/ijepa...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='import argparse, torch\\r\\nfrom torchvision import utils\\r\\nfrom stylegan2 import Generator\\r\\nfrom tqdm import tqdm\\r\\n\\r\\ndef generate(args, g_ema, device, mean_latent):\\r\\n    with torch.no_grad():\\r\\n        g_ema.eval()\\r\\n        for i in tqdm(range(args.pics)):\\r\\n            sample_z = torch.randn(args.sample, args.latent, device=device)\\r\\n\\r\\n            sample, _, _ = g_ema(\\r\\n                [sample_z], truncation=args.truncation, truncation_latent=mean_latent\\r\\n            )\\r\\n\\r\\n            utils.save_image(\\r\\n                sample,\\r\\n                f\"sample/{str(i).zfill(6)}.png\",\\r\\n                nrow=1,\\r\\n                normalize=True,\\r\\n                range=(-1, 1),\\r\\n            )\\r\\n\\r\\nif __name__ == \"__main__\":\\r\\n    parser = argparse.ArgumentParser(description=\"Generate samples from the generator\")\\r\\n\\r\\n    parser.add_argument(\\r\\n        \"--size\", type=int, default=256, help=\"output image size of the generator\"\\r\\n    )\\r\\n    parser.add_argument(\\r\\n        \"--device\", type=str, default=\\'cuda\\', help=\"output image size of the generator\"\\r\\n    )\\r\\n    parser.add_argument(\\r\\n        \"--sample\",\\r\\n        type=int,\\r\\n        default=1,\\r\\n        help=\"number of samples to be generated for each image\",\\r\\n    )\\r\\n    parser.add_argument(\\r\\n        \"--pics\", type=int, default=20, help=\"number of images to be generated\"\\r\\n    )\\r\\n    parser.add_argument(\"--truncation\", type=float, default=1, help=\"truncation ratio\")\\r\\n    parser.add_argument(\\r\\n        \"--truncation_mean\",\\r\\n        type=int,\\r\\n        default=4096,\\r\\n        help=\"number of vectors to calculate mean for the truncation\",\\r\\n    )\\r\\n    parser.add_argument(\\r\\n        \"--ckpt\",\\r\\n        type=str,\\r\\n        default=\"stylegan2-ffhq-config-f.pt\",\\r\\n        help=\"path to the model checkpoint\",\\r\\n    )\\r\\n    parser.add_argument(\\r\\n        \"--channel_multiplier\",\\r\\n        type=int,\\r\\n        default=2,\\r\\n        help=\"channel multiplier of the generator. config-f = 2, else = 1\",\\r\\n    )\\r\\n\\r\\n    args = parser.parse_args()\\r\\n\\r\\n    args.latent = 512\\r\\n    args.n_mlp = 8\\r\\n    device = torch.device(args.device)\\r\\n\\r\\n    g_ema = Generator(\\r\\n        args.size, args.latent, args.n_mlp, channel_multiplier=args.channel_multiplier\\r\\n    ).to(device)\\r\\n    checkpoint = torch.load(args.ckpt, map_location=device)\\r\\n\\r\\n    g_ema.load_state_dict(checkpoint[\"g_ema\"], strict=False)\\r\\n\\r\\n    if args.truncation < 1:\\r\\n        with torch.no_grad():\\r\\n            mean_latent = g_ema.mean_latent(args.truncation_mean)\\r\\n    else:\\r\\n        mean_latent = None\\r\\n\\r\\n    generate(args, g_ema, device, mean_latent)\\r\\n', metadata={'file_path': 'generate.py', 'file_name': 'generate.py', 'file_type': '.py'}),\n",
       " Document(page_content='import contextlib\\r\\nimport warnings\\r\\n\\r\\nimport torch\\r\\nfrom torch import autograd\\r\\nfrom torch.nn import functional as F\\r\\n\\r\\nenabled = True\\r\\nweight_gradients_disabled = False\\r\\n\\r\\n\\r\\n@contextlib.contextmanager\\r\\ndef no_weight_gradients():\\r\\n    global weight_gradients_disabled\\r\\n\\r\\n    old = weight_gradients_disabled\\r\\n    weight_gradients_disabled = True\\r\\n    yield\\r\\n    weight_gradients_disabled = old\\r\\n\\r\\n\\r\\ndef conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):\\r\\n    if could_use_op(input):\\r\\n        return conv2d_gradfix(\\r\\n            transpose=False,\\r\\n            weight_shape=weight.shape,\\r\\n            stride=stride,\\r\\n            padding=padding,\\r\\n            output_padding=0,\\r\\n            dilation=dilation,\\r\\n            groups=groups,\\r\\n        ).apply(input, weight, bias)\\r\\n\\r\\n    return F.conv2d(\\r\\n        input=input,\\r\\n        weight=weight,\\r\\n        bias=bias,\\r\\n        stride=stride,\\r\\n        padding=padding,\\r\\n        dilation=dilation,\\r\\n        groups=groups,\\r\\n    )\\r\\n\\r\\n\\r\\ndef conv_transpose2d(\\r\\n    input,\\r\\n    weight,\\r\\n    bias=None,\\r\\n    stride=1,\\r\\n    padding=0,\\r\\n    output_padding=0,\\r\\n    groups=1,\\r\\n    dilation=1,\\r\\n):\\r\\n    if could_use_op(input):\\r\\n        return conv2d_gradfix(\\r\\n            transpose=True,\\r\\n            weight_shape=weight.shape,\\r\\n            stride=stride,\\r\\n            padding=padding,\\r\\n            output_padding=output_padding,\\r\\n            groups=groups,\\r\\n            dilation=dilation,\\r\\n        ).apply(input, weight, bias)\\r\\n\\r\\n    return F.conv_transpose2d(\\r\\n        input=input,\\r\\n        weight=weight,\\r\\n        bias=bias,\\r\\n        stride=stride,\\r\\n        padding=padding,\\r\\n        output_padding=output_padding,\\r\\n        dilation=dilation,\\r\\n        groups=groups,\\r\\n    )\\r\\n\\r\\n\\r\\ndef could_use_op(input):\\r\\n    if (not enabled) or (not torch.backends.cudnn.enabled):\\r\\n        return False\\r\\n\\r\\n    if input.device.type != \"cuda\":\\r\\n        return False\\r\\n\\r\\n    if any(torch.__version__.startswith(x) for x in [\"1.7.\", \"1.8.\"]):\\r\\n        return True\\r\\n\\r\\n    warnings.warn(\\r\\n        f\"conv2d_gradfix not supported on PyTorch {torch.__version__}. Falling back to torch.nn.functional.conv2d().\"\\r\\n    )\\r\\n\\r\\n    return False\\r\\n\\r\\n\\r\\ndef ensure_tuple(xs, ndim):\\r\\n    xs = tuple(xs) if isinstance(xs, (tuple, list)) else (xs,) * ndim\\r\\n\\r\\n    return xs\\r\\n\\r\\n\\r\\nconv2d_gradfix_cache = dict()\\r\\n\\r\\n\\r\\ndef conv2d_gradfix(\\r\\n    transpose, weight_shape, stride, padding, output_padding, dilation, groups\\r\\n):\\r\\n    ndim = 2\\r\\n    weight_shape = tuple(weight_shape)\\r\\n    stride = ensure_tuple(stride, ndim)\\r\\n    padding = ensure_tuple(padding, ndim)\\r\\n    output_padding = ensure_tuple(output_padding, ndim)\\r\\n    dilation = ensure_tuple(dilation, ndim)\\r\\n\\r\\n    key = (transpose, weight_shape, stride, padding, output_padding, dilation, groups)\\r\\n    if key in conv2d_gradfix_cache:\\r\\n        return conv2d_gradfix_cache[key]\\r\\n\\r\\n    common_kwargs = dict(\\r\\n        stride=stride, padding=padding, dilation=dilation, groups=groups\\r\\n    )\\r\\n\\r\\n    def calc_output_padding(input_shape, output_shape):\\r\\n        if transpose:\\r\\n            return [0, 0]\\r\\n\\r\\n        return [\\r\\n            input_shape[i + 2]\\r\\n            - (output_shape[i + 2] - 1) * stride[i]\\r\\n            - (1 - 2 * padding[i])\\r\\n            - dilation[i] * (weight_shape[i + 2] - 1)\\r\\n            for i in range(ndim)\\r\\n        ]\\r\\n\\r\\n    class Conv2d(autograd.Function):\\r\\n        @staticmethod\\r\\n        def forward(ctx, input, weight, bias):\\r\\n            if not transpose:\\r\\n                out = F.conv2d(input=input, weight=weight, bias=bias, **common_kwargs)\\r\\n\\r\\n            else:\\r\\n                out = F.conv_transpose2d(\\r\\n                    input=input,\\r\\n                    weight=weight,\\r\\n                    bias=bias,\\r\\n                    output_padding=output_padding,\\r\\n                    **common_kwargs,\\r\\n                )\\r\\n\\r\\n            ctx.save_for_backward(input, weight)\\r\\n\\r\\n            return out\\r\\n\\r\\n        @staticmethod\\r\\n        def backward(ctx, grad_output):\\r\\n            input, weight = ctx.saved_tensors\\r\\n            grad_input, grad_weight, grad_bias = None, None, None\\r\\n\\r\\n            if ctx.needs_input_grad[0]:\\r\\n                p = calc_output_padding(\\r\\n                    input_shape=input.shape, output_shape=grad_output.shape\\r\\n                )\\r\\n                grad_input = conv2d_gradfix(\\r\\n                    transpose=(not transpose),\\r\\n                    weight_shape=weight_shape,\\r\\n                    output_padding=p,\\r\\n                    **common_kwargs,\\r\\n                ).apply(grad_output, weight, None)\\r\\n\\r\\n            if ctx.needs_input_grad[1] and not weight_gradients_disabled:\\r\\n                grad_weight = Conv2dGradWeight.apply(grad_output, input)\\r\\n\\r\\n            if ctx.needs_input_grad[2]:\\r\\n                grad_bias = grad_output.sum((0, 2, 3))\\r\\n\\r\\n            return grad_input, grad_weight, grad_bias\\r\\n\\r\\n    class Conv2dGradWeight(autograd.Function):\\r\\n        @staticmethod\\r\\n        def forward(ctx, grad_output, input):\\r\\n            op = torch._C._jit_get_operation(\\r\\n                \"aten::cudnn_convolution_backward_weight\"\\r\\n                if not transpose\\r\\n                else \"aten::cudnn_convolution_transpose_backward_weight\"\\r\\n            )\\r\\n            flags = [\\r\\n                torch.backends.cudnn.benchmark,\\r\\n                torch.backends.cudnn.deterministic,\\r\\n                torch.backends.cudnn.allow_tf32,\\r\\n            ]\\r\\n            grad_weight = op(\\r\\n                weight_shape,\\r\\n                grad_output,\\r\\n                input,\\r\\n                padding,\\r\\n                stride,\\r\\n                dilation,\\r\\n                groups,\\r\\n                *flags,\\r\\n            )\\r\\n            ctx.save_for_backward(grad_output, input)\\r\\n\\r\\n            return grad_weight\\r\\n\\r\\n        @staticmethod\\r\\n        def backward(ctx, grad_grad_weight):\\r\\n            grad_output, input = ctx.saved_tensors\\r\\n            grad_grad_output, grad_grad_input = None, None\\r\\n\\r\\n            if ctx.needs_input_grad[0]:\\r\\n                grad_grad_output = Conv2d.apply(input, grad_grad_weight, None)\\r\\n\\r\\n            if ctx.needs_input_grad[1]:\\r\\n                p = calc_output_padding(\\r\\n                    input_shape=input.shape, output_shape=grad_output.shape\\r\\n                )\\r\\n                grad_grad_input = conv2d_gradfix(\\r\\n                    transpose=(not transpose),\\r\\n                    weight_shape=weight_shape,\\r\\n                    output_padding=p,\\r\\n                    **common_kwargs,\\r\\n                ).apply(grad_output, grad_grad_weight, None)\\r\\n\\r\\n            return grad_grad_output, grad_grad_input\\r\\n\\r\\n    conv2d_gradfix_cache[key] = Conv2d\\r\\n\\r\\n    return Conv2d\\r\\n', metadata={'file_path': 'op\\\\conv2d_gradfix.py', 'file_name': 'conv2d_gradfix.py', 'file_type': '.py'}),\n",
       " Document(page_content='import torch, math\\r\\nimport numpy as np\\r\\nfrom stylegan2 import Generator\\r\\nimport torch.nn.functional as functional\\r\\n\\r\\ndef linear(feature, p0, p1, d, axis=0):\\r\\n    f0 = feature[..., p0[0], p0[1]]\\r\\n    f1 = feature[..., p1[0], p1[1]]\\r\\n    weight = abs(d[axis])\\r\\n    f = (1 - weight) * f0 + weight * f1\\r\\n    return f\\r\\n\\r\\ndef bilinear(feature, qi, d):\\r\\n    y0, x0 = qi\\r\\n    dy, dx = d\\r\\n    d = (dx, dy)\\r\\n    dx = 1 if dx >= 0 else -1\\r\\n    dy = 1 if dy >= 0 else -1\\r\\n    x1 = x0 + dx\\r\\n    y1 = y0 + dy\\r\\n    fx1 = linear(feature, (x0, y0), (x1, y0), d, axis=0)\\r\\n    fx2 = linear(feature, (x0, y1), (x1, y1), d, axis=0)\\r\\n    weight = abs(d[1])\\r\\n    fx = (1 - weight) * fx1 + weight * fx2\\r\\n    return fx\\r\\n\\r\\ndef motion_supervision(F0, F, pi, ti, r1=3, M=None):\\r\\n    F = functional.interpolate(F, [256, 256], mode=\"bilinear\")\\r\\n    F0 = functional.interpolate(F0, [256, 256], mode=\"bilinear\")\\r\\n\\r\\n    dw, dh = ti[0] - pi[0], ti[1] - pi[1]\\r\\n    norm = math.sqrt(dw**2 + dh**2)\\r\\n    w = (max(0, pi[0] - r1), min(256, pi[0] + r1))\\r\\n    h = (max(0, pi[1] - r1), min(256, pi[1] + r1))\\r\\n    d = torch.tensor(\\r\\n        (dw / norm, dh / norm),\\r\\n        dtype=F.dtype, device=F.device,\\r\\n    ).reshape(1, 1, 1, 2)\\r\\n    grid_h, grid_w = torch.meshgrid(\\r\\n        torch.tensor(range(h[0], h[1])),\\r\\n        torch.tensor(range(w[0], w[1])),\\r\\n        indexing=\\'xy\\',\\r\\n    )\\r\\n    grid = torch.stack([grid_w, grid_h], dim=-1).unsqueeze(0)\\r\\n    grid = (grid / 255 - 0.5) * 2\\r\\n    grid_d = grid + 2 * d / 255\\r\\n\\r\\n    sample = functional.grid_sample(\\r\\n        F, grid, mode=\\'bilinear\\', padding_mode=\\'border\\',\\r\\n        align_corners=True,\\r\\n    )\\r\\n    sample_d = functional.grid_sample(\\r\\n        F, grid_d, mode=\\'bilinear\\', padding_mode=\\'border\\',\\r\\n        align_corners=True,\\r\\n    )\\r\\n\\r\\n    loss = (sample_d - sample.detach()).abs().mean(1).sum()\\r\\n\\r\\n    return loss\\r\\n\\r\\n@torch.no_grad()\\r\\ndef point_tracking(F0, F, pi, p0, r2=12):\\r\\n    F = functional.interpolate(F, [256, 256], mode=\"bilinear\")\\r\\n    F0 = functional.interpolate(F0, [256, 256], mode=\"bilinear\")\\r\\n    x = (max(0, pi[0] - r2), min(256, pi[0] + r2))\\r\\n    y = (max(0, pi[1] - r2), min(256, pi[1] + r2))\\r\\n    base = F0[..., p0[1], p0[0]].reshape(1, -1, 1, 1)\\r\\n    diff = (F[..., y[0]:y[1], x[0]:x[1]] - base).abs().mean(1)\\r\\n    idx = diff.argmin()\\r\\n    dy = int(idx / (x[1] - x[0]))\\r\\n    dx = int(idx % (x[1] - x[0]))\\r\\n    npi = (x[0] + dx, y[0] + dy)\\r\\n    return npi\\r\\n\\r\\ndef requires_grad(model, flag=True):\\r\\n    for p in model.parameters():\\r\\n        p.requires_grad = flag\\r\\n\\r\\nclass DragGAN():\\r\\n    def __init__(self, device, layer_index=6):\\r\\n        self.generator = Generator(256, 512, 8).to(device)\\r\\n        requires_grad(self.generator, False)\\r\\n        self._device = device\\r\\n        self.layer_index = layer_index\\r\\n        self.latent = None\\r\\n        self.F0 = None\\r\\n        self.optimizer = None\\r\\n        self.p0 = None\\r\\n\\r\\n    def load_ckpt(self, path):\\r\\n        print(f\\'loading checkpoint from {path}\\')\\r\\n        ckpt = torch.load(path, map_location=self._device)\\r\\n        self.generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\\r\\n        print(\\'loading checkpoint successed!\\')\\r\\n\\r\\n    def to(self, device):\\r\\n        if self._device != device:\\r\\n            self.generator = self.generator.to(device)\\r\\n            self._device = device\\r\\n\\r\\n    @torch.no_grad()\\r\\n    def generate_image(self, seed):\\r\\n        z = torch.from_numpy(\\r\\n            np.random.RandomState(seed).randn(1, 512).astype(np.float32)\\r\\n        ).to(self._device)\\r\\n        image, self.latent, self.F0 = self.generator(\\r\\n            [z], return_latents=True, return_features=True, randomize_noise=False,\\r\\n        )\\r\\n        image, self.F0 = image[0], self.F0[self.layer_index*2+1].detach()\\r\\n        image = image.detach().cpu().permute(1, 2, 0).numpy()\\r\\n        image = (image / 2 + 0.5).clip(0, 1).reshape(-1)\\r\\n        return image\\r\\n\\r\\n    @property\\r\\n    def device(self):\\r\\n        return self._device\\r\\n\\r\\n    def __call__(self, *args, **kwargs):\\r\\n        return self.generator(*args, **kwargs)\\r\\n\\r\\n    def step(self, points):\\r\\n        if self.optimizer is None:\\r\\n            len_pts = (len(points) // 2) * 2\\r\\n            if len_pts == 0:\\r\\n                print(\\'Select at least one pair of points\\')\\r\\n                return False, None\\r\\n            self.trainable = self.latent[:, :self.layer_index*2, :].detach(\\r\\n            ).requires_grad_(True)\\r\\n            self.fixed = self.latent[:, self.layer_index*2:, :].detach(\\r\\n            ).requires_grad_(False)\\r\\n            self.optimizer = torch.optim.Adam([self.trainable], lr=2e-3)\\r\\n            points = points[:len_pts]\\r\\n            self.p0 = points[::2]\\r\\n        self.optimizer.zero_grad()\\r\\n        trainable_fixed = torch.cat([self.trainable, self.fixed], dim=1)\\r\\n        image, _, features = self.generator(\\r\\n            [trainable_fixed], input_is_latent=True,\\r\\n            return_features=True, randomize_noise=False,\\r\\n        )\\r\\n        features = features[self.layer_index*2+1]\\r\\n        loss = 0\\r\\n        for i in range(len(self.p0)):\\r\\n            loss += motion_supervision(self.F0, features, points[2*i], points[2*i+1])\\r\\n        print(loss)\\r\\n        loss.backward()\\r\\n        self.optimizer.step()\\r\\n        image, _, features = self.generator(\\r\\n            [trainable_fixed], input_is_latent=True,\\r\\n            return_features=True, randomize_noise=False,\\r\\n        )\\r\\n        features = features[self.layer_index*2+1]\\r\\n        image = image[0].detach().cpu().permute(1, 2, 0).numpy()\\r\\n        image = (image / 2 + 0.5).clip(0, 1).reshape(-1)\\r\\n        for i in range(len(self.p0)):\\r\\n            points[2*i] = point_tracking(self.F0, features, points[2*i], self.p0[i])\\r\\n        return True, (points, image)\\r\\n', metadata={'file_path': 'draggan.py', 'file_name': 'draggan.py', 'file_type': '.py'}),\n",
       " Document(page_content='import dearpygui.dearpygui as dpg\\r\\nimport numpy as np\\r\\nfrom draggan import DragGAN\\r\\nfrom array import array\\r\\nimport threading\\r\\n\\r\\nadd_point = 0\\r\\npoint_color = [(1, 0, 0), (0, 0, 1)]\\r\\npoints, steps = [], 0\\r\\ndragging = False\\r\\n# mvFormat_Float_rgb not currently supported on macOS\\r\\n# More details: https://dearpygui.readthedocs.io/en/latest/documentation/textures.html#formats\\r\\ntexture_format = dpg.mvFormat_Float_rgba\\r\\nimage_width, image_height, rgb_channel, rgba_channel = 256, 256, 3, 4\\r\\nimage_pixels = image_height * image_width\\r\\nmodel = DragGAN(\\'cpu\\')\\r\\n\\r\\ndpg.create_context()\\r\\ndpg.create_viewport(title=\\'DragGAN\\', width=800, height=650)\\r\\n\\r\\nraw_data_size = image_width * image_height * rgba_channel\\r\\nraw_data = array(\\'f\\', [1] * raw_data_size)\\r\\nwith dpg.texture_registry(show=False):\\r\\n    dpg.add_raw_texture(\\r\\n        width=image_width, height=image_height, default_value=raw_data,\\r\\n        format=texture_format, tag=\"image\"\\r\\n    )\\r\\n\\r\\ndef update_image(new_image):\\r\\n    # Convert image data (rgb) to raw_data (rgba)\\r\\n    for i in range(0, image_pixels):\\r\\n        rd_base, im_base = i * rgba_channel, i * rgb_channel\\r\\n        raw_data[rd_base:rd_base + rgb_channel] = array(\\r\\n            \\'f\\', new_image[im_base:im_base + rgb_channel]\\r\\n        )\\r\\n\\r\\ndef generate_image(sender, app_data, user_data):\\r\\n    seed = dpg.get_value(\\'seed\\')\\r\\n    image = model.generate_image(seed)\\r\\n    update_image(image)\\r\\n\\r\\ndef change_device(sender, app_data):\\r\\n    model.to(app_data)\\r\\n\\r\\ndef dragging_thread():\\r\\n    global points, steps, dragging\\r\\n    while (dragging):\\r\\n        status, ret = model.step(points)\\r\\n        if status:\\r\\n            points, image = ret\\r\\n        else:\\r\\n            dragging = False\\r\\n            return\\r\\n        update_image(image)\\r\\n        for i in range(len(points)):\\r\\n            draw_point(*points[i], point_color[i%2])\\r\\n        steps += 1\\r\\n        dpg.set_value(\\'steps\\', f\\'steps: {steps}\\')\\r\\n\\r\\nwidth, height = 260, 200\\r\\nposx, posy = 0, 0\\r\\nwith dpg.window(\\r\\n    label=\\'Network & Latent\\', width=width, height=height, pos=(posx, posy),\\r\\n    no_move=True, no_close=True, no_collapse=True, no_resize=True,\\r\\n):\\r\\n    dpg.add_text(\\'device\\', pos=(5, 20))\\r\\n    dpg.add_combo(\\r\\n        (\\'cpu\\', \\'cuda\\'), default_value=\\'cpu\\', width=60, pos=(70, 20),\\r\\n        callback=change_device,\\r\\n    )\\r\\n\\r\\n    dpg.add_text(\\'weight\\', pos=(5, 40))\\r\\n\\r\\n    def select_cb(sender, app_data):\\r\\n        selections = app_data[\\'selections\\']\\r\\n        if selections:\\r\\n            for fn in selections:\\r\\n                model.load_ckpt(selections[fn])\\r\\n                break\\r\\n\\r\\n    def cancel_cb(sender, app_data):\\r\\n        ...\\r\\n\\r\\n    with dpg.file_dialog(\\r\\n        directory_selector=False, show=False, callback=select_cb, id=\\'weight selector\\',\\r\\n        cancel_callback=cancel_cb, width=700 ,height=400\\r\\n    ):\\r\\n        dpg.add_file_extension(\\'.*\\')\\r\\n    dpg.add_button(\\r\\n        label=\"select weight\", callback=lambda: dpg.show_item(\"weight selector\"),\\r\\n        pos=(70, 40),\\r\\n    )\\r\\n\\r\\n    dpg.add_text(\\'latent\\', pos=(5, 60))\\r\\n    dpg.add_input_int(\\r\\n        label=\\'seed\\', width=100, pos=(70, 60), tag=\\'seed\\', default_value=512,\\r\\n    )\\r\\n    dpg.add_input_float(\\r\\n        label=\\'step size\\', width=54, pos=(70, 80), step=-1, default_value=0.002,\\r\\n    )\\r\\n    dpg.add_button(label=\"reset\", width=54, pos=(70, 100), callback=None)\\r\\n    dpg.add_radio_button(\\r\\n        items=(\\'w\\', \\'w+\\'), pos=(130, 100), horizontal=True, default_value=\\'w+\\',\\r\\n    )\\r\\n    dpg.add_button(label=\"generate\", pos=(70, 120), callback=generate_image)\\r\\n\\r\\nposy += height + 2\\r\\nwith dpg.window(\\r\\n    label=\\'Drag\\', width=width, height=height, pos=(posx, posy),\\r\\n    no_move=True, no_close=True, no_collapse=True, no_resize=True,\\r\\n):\\r\\n    def add_point_cb():\\r\\n        global add_point\\r\\n        add_point += 2\\r\\n\\r\\n    def reset_point_cb():\\r\\n        global points\\r\\n        points = []\\r\\n\\r\\n    def start_cb():\\r\\n        global dragging\\r\\n        if dragging: return\\r\\n        dragging = True\\r\\n        threading.Thread(target=dragging_thread).start()\\r\\n\\r\\n    def stop_cb():\\r\\n        global dragging\\r\\n        dragging = False\\r\\n        print(\\'stop dragging...\\')\\r\\n\\r\\n    dpg.add_text(\\'drag\\', pos=(5, 20))\\r\\n    dpg.add_button(label=\"add point\", width=80, pos=(70, 20), callback=add_point_cb)\\r\\n    dpg.add_button(label=\"reset point\", width=80, pos=(155, 20), callback=reset_point_cb)\\r\\n    dpg.add_button(label=\"start\", width=80, pos=(70, 40), callback=start_cb)\\r\\n    dpg.add_button(label=\"stop\", width=80, pos=(155, 40), callback=stop_cb)\\r\\n    dpg.add_text(\\'steps: 0\\', tag=\\'steps\\', pos=(70, 60))\\r\\n\\r\\n    dpg.add_text(\\'mask\\', pos=(5, 80))\\r\\n    dpg.add_button(label=\"fixed area\", width=80, pos=(70, 80), callback=None)\\r\\n    dpg.add_button(label=\"reset mask\", width=80, pos=(70, 100), callback=None)\\r\\n    dpg.add_checkbox(label=\\'show mask\\', pos=(155, 100), default_value=False)\\r\\n    dpg.add_input_int(label=\\'radius\\', width=100, pos=(70, 120), default_value=50)\\r\\n    dpg.add_input_float(label=\\'lambda\\', width=100, pos=(70, 140), default_value=20)\\r\\n\\r\\nposy += height + 2\\r\\nwith dpg.window(\\r\\n    label=\\'Capture\\', width=width, height=height, pos=(posx, posy),\\r\\n    no_move=True, no_close=True, no_collapse=True, no_resize=True,\\r\\n):\\r\\n    dpg.add_text(\\'capture\\', pos=(5, 20))\\r\\n    dpg.add_input_text(pos=(70, 20), default_value=\\'capture\\')\\r\\n    dpg.add_button(label=\"save image\", width=80, pos=(70, 40), callback=None)\\r\\n\\r\\ndef draw_point(x, y, color):\\r\\n    x_start, x_end = max(0, x - 2), min(image_width, x + 2)\\r\\n    y_start, y_end = max(0, y - 2), min(image_height, y + 2)\\r\\n    for x in range(x_start, x_end):\\r\\n        for y in range(y_start, y_end):\\r\\n            offset = (y * image_width + x) * rgba_channel\\r\\n            raw_data[offset:offset + rgb_channel] = array(\\'f\\', color[:rgb_channel])\\r\\n\\r\\ndef select_point(sender, app_data):\\r\\n    global add_point, points\\r\\n    if add_point <= 0: return\\r\\n    ms_pos = dpg.get_mouse_pos(local=False)\\r\\n    id_pos = dpg.get_item_pos(\\'image_data\\')\\r\\n    iw_pos = dpg.get_item_pos(\\'Image Win\\')\\r\\n    ix = int(ms_pos[0]-id_pos[0]-iw_pos[0])\\r\\n    iy = int(ms_pos[1]-id_pos[1]-iw_pos[1])\\r\\n    draw_point(ix, iy, point_color[add_point % 2])\\r\\n    points.append(np.array([ix, iy]))\\r\\n    print(points)\\r\\n    add_point -= 1\\r\\n\\r\\nposx, posy = 2 + width, 0\\r\\nwith dpg.window(\\r\\n    label=\\'Image\\', pos=(posx, posy), tag=\\'Image Win\\',\\r\\n    no_move=True, no_close=True, no_collapse=True, no_resize=True,\\r\\n):\\r\\n    dpg.add_image(\"image\", show=True, tag=\\'image_data\\', pos=(10, 30))\\r\\n\\r\\nwith dpg.item_handler_registry(tag=\\'double_clicked_handler\\'):\\r\\n    dpg.add_item_double_clicked_handler(callback=select_point)\\r\\ndpg.bind_item_handler_registry(\"image_data\", \"double_clicked_handler\")\\r\\n\\r\\ndpg.setup_dearpygui()\\r\\ndpg.show_viewport()\\r\\ndpg.start_dearpygui()\\r\\ndpg.destroy_context()\\r\\n', metadata={'file_path': 'gui.py', 'file_name': 'gui.py', 'file_type': '.py'}),\n",
       " Document(page_content='import os\\r\\n\\r\\nimport torch\\r\\nfrom torch import nn\\r\\nfrom torch.nn import functional as F\\r\\n\\r\\n\\r\\nclass FusedLeakyReLU(nn.Module):\\r\\n    def __init__(self, channel, bias=True, negative_slope=0.2, scale=2 ** 0.5):\\r\\n        super().__init__()\\r\\n\\r\\n        if bias:\\r\\n            self.bias = nn.Parameter(torch.zeros(channel))\\r\\n\\r\\n        else:\\r\\n            self.bias = None\\r\\n\\r\\n        self.negative_slope = negative_slope\\r\\n        self.scale = scale\\r\\n\\r\\n    def forward(self, input):\\r\\n        return fused_leaky_relu(input, self.bias, self.negative_slope, self.scale)\\r\\n\\r\\n\\r\\ndef fused_leaky_relu(input, bias=None, negative_slope=0.2, scale=2 ** 0.5):\\r\\n    if bias is not None:\\r\\n        rest_dim = [1] * (input.ndim - bias.ndim - 1)\\r\\n        return (\\r\\n            F.leaky_relu(\\r\\n                input + bias.view(1, bias.shape[0], *rest_dim), negative_slope=0.2\\r\\n            )\\r\\n            * scale\\r\\n        )\\r\\n\\r\\n    else:\\r\\n        return F.leaky_relu(input, negative_slope=0.2) * scale\\r\\n', metadata={'file_path': 'op\\\\fused_act.py', 'file_name': 'fused_act.py', 'file_type': '.py'}),\n",
       " Document(page_content='import math\\r\\nimport random\\r\\nimport functools\\r\\nimport operator\\r\\n\\r\\nimport torch\\r\\nfrom torch import nn\\r\\nfrom torch.nn import functional as F\\r\\nfrom torch.autograd import Function\\r\\n\\r\\nfrom op import FusedLeakyReLU, fused_leaky_relu, upfirdn2d, conv2d_gradfix\\r\\n\\r\\n\\r\\nclass PixelNorm(nn.Module):\\r\\n    def __init__(self):\\r\\n        super().__init__()\\r\\n\\r\\n    def forward(self, input):\\r\\n        return input * torch.rsqrt(torch.mean(input ** 2, dim=1, keepdim=True) + 1e-8)\\r\\n\\r\\n\\r\\ndef make_kernel(k):\\r\\n    k = torch.tensor(k, dtype=torch.float32)\\r\\n\\r\\n    if k.ndim == 1:\\r\\n        k = k[None, :] * k[:, None]\\r\\n\\r\\n    k /= k.sum()\\r\\n\\r\\n    return k\\r\\n\\r\\n\\r\\nclass Upsample(nn.Module):\\r\\n    def __init__(self, kernel, factor=2):\\r\\n        super().__init__()\\r\\n\\r\\n        self.factor = factor\\r\\n        kernel = make_kernel(kernel) * (factor ** 2)\\r\\n        self.register_buffer(\"kernel\", kernel)\\r\\n\\r\\n        p = kernel.shape[0] - factor\\r\\n\\r\\n        pad0 = (p + 1) // 2 + factor - 1\\r\\n        pad1 = p // 2\\r\\n\\r\\n        self.pad = (pad0, pad1)\\r\\n\\r\\n    def forward(self, input):\\r\\n        out = upfirdn2d(input, self.kernel, up=self.factor, down=1, pad=self.pad)\\r\\n\\r\\n        return out\\r\\n\\r\\n\\r\\nclass Downsample(nn.Module):\\r\\n    def __init__(self, kernel, factor=2):\\r\\n        super().__init__()\\r\\n\\r\\n        self.factor = factor\\r\\n        kernel = make_kernel(kernel)\\r\\n        self.register_buffer(\"kernel\", kernel)\\r\\n\\r\\n        p = kernel.shape[0] - factor\\r\\n\\r\\n        pad0 = (p + 1) // 2\\r\\n        pad1 = p // 2\\r\\n\\r\\n        self.pad = (pad0, pad1)\\r\\n\\r\\n    def forward(self, input):\\r\\n        out = upfirdn2d(input, self.kernel, up=1, down=self.factor, pad=self.pad)\\r\\n\\r\\n        return out\\r\\n\\r\\n\\r\\nclass Blur(nn.Module):\\r\\n    def __init__(self, kernel, pad, upsample_factor=1):\\r\\n        super().__init__()\\r\\n\\r\\n        kernel = make_kernel(kernel)\\r\\n\\r\\n        if upsample_factor > 1:\\r\\n            kernel = kernel * (upsample_factor ** 2)\\r\\n\\r\\n        self.register_buffer(\"kernel\", kernel)\\r\\n\\r\\n        self.pad = pad\\r\\n\\r\\n    def forward(self, input):\\r\\n        out = upfirdn2d(input, self.kernel, pad=self.pad)\\r\\n\\r\\n        return out\\r\\n\\r\\n\\r\\nclass EqualConv2d(nn.Module):\\r\\n    def __init__(\\r\\n        self, in_channel, out_channel, kernel_size, stride=1, padding=0, bias=True\\r\\n    ):\\r\\n        super().__init__()\\r\\n\\r\\n        self.weight = nn.Parameter(\\r\\n            torch.randn(out_channel, in_channel, kernel_size, kernel_size)\\r\\n        )\\r\\n        self.scale = 1 / math.sqrt(in_channel * kernel_size ** 2)\\r\\n\\r\\n        self.stride = stride\\r\\n        self.padding = padding\\r\\n\\r\\n        if bias:\\r\\n            self.bias = nn.Parameter(torch.zeros(out_channel))\\r\\n\\r\\n        else:\\r\\n            self.bias = None\\r\\n\\r\\n    def forward(self, input):\\r\\n        out = conv2d_gradfix.conv2d(\\r\\n            input,\\r\\n            self.weight * self.scale,\\r\\n            bias=self.bias,\\r\\n            stride=self.stride,\\r\\n            padding=self.padding,\\r\\n        )\\r\\n\\r\\n        return out\\r\\n\\r\\n    def __repr__(self):\\r\\n        return (\\r\\n            f\"{self.__class__.__name__}({self.weight.shape[1]}, {self.weight.shape[0]},\"\\r\\n            f\" {self.weight.shape[2]}, stride={self.stride}, padding={self.padding})\"\\r\\n        )\\r\\n\\r\\n\\r\\nclass EqualLinear(nn.Module):\\r\\n    def __init__(\\r\\n        self, in_dim, out_dim, bias=True, bias_init=0, lr_mul=1, activation=None\\r\\n    ):\\r\\n        super().__init__()\\r\\n\\r\\n        self.weight = nn.Parameter(torch.randn(out_dim, in_dim).div_(lr_mul))\\r\\n\\r\\n        if bias:\\r\\n            self.bias = nn.Parameter(torch.zeros(out_dim).fill_(bias_init))\\r\\n\\r\\n        else:\\r\\n            self.bias = None\\r\\n\\r\\n        self.activation = activation\\r\\n\\r\\n        self.scale = (1 / math.sqrt(in_dim)) * lr_mul\\r\\n        self.lr_mul = lr_mul\\r\\n\\r\\n    def forward(self, input):\\r\\n        if self.activation:\\r\\n            out = F.linear(input, self.weight * self.scale)\\r\\n            out = fused_leaky_relu(out, self.bias * self.lr_mul)\\r\\n\\r\\n        else:\\r\\n            out = F.linear(\\r\\n                input, self.weight * self.scale, bias=self.bias * self.lr_mul\\r\\n            )\\r\\n\\r\\n        return out\\r\\n\\r\\n    def __repr__(self):\\r\\n        return (\\r\\n            f\"{self.__class__.__name__}({self.weight.shape[1]}, {self.weight.shape[0]})\"\\r\\n        )\\r\\n\\r\\n\\r\\nclass ModulatedConv2d(nn.Module):\\r\\n    def __init__(\\r\\n        self,\\r\\n        in_channel,\\r\\n        out_channel,\\r\\n        kernel_size,\\r\\n        style_dim,\\r\\n        demodulate=True,\\r\\n        upsample=False,\\r\\n        downsample=False,\\r\\n        blur_kernel=[1, 3, 3, 1],\\r\\n        fused=True,\\r\\n    ):\\r\\n        super().__init__()\\r\\n\\r\\n        self.eps = 1e-8\\r\\n        self.kernel_size = kernel_size\\r\\n        self.in_channel = in_channel\\r\\n        self.out_channel = out_channel\\r\\n        self.upsample = upsample\\r\\n        self.downsample = downsample\\r\\n\\r\\n        if upsample:\\r\\n            factor = 2\\r\\n            p = (len(blur_kernel) - factor) - (kernel_size - 1)\\r\\n            pad0 = (p + 1) // 2 + factor - 1\\r\\n            pad1 = p // 2 + 1\\r\\n\\r\\n            self.blur = Blur(blur_kernel, pad=(pad0, pad1), upsample_factor=factor)\\r\\n\\r\\n        if downsample:\\r\\n            factor = 2\\r\\n            p = (len(blur_kernel) - factor) + (kernel_size - 1)\\r\\n            pad0 = (p + 1) // 2\\r\\n            pad1 = p // 2\\r\\n\\r\\n            self.blur = Blur(blur_kernel, pad=(pad0, pad1))\\r\\n\\r\\n        fan_in = in_channel * kernel_size ** 2\\r\\n        self.scale = 1 / math.sqrt(fan_in)\\r\\n        self.padding = kernel_size // 2\\r\\n\\r\\n        self.weight = nn.Parameter(\\r\\n            torch.randn(1, out_channel, in_channel, kernel_size, kernel_size)\\r\\n        )\\r\\n\\r\\n        self.modulation = EqualLinear(style_dim, in_channel, bias_init=1)\\r\\n\\r\\n        self.demodulate = demodulate\\r\\n        self.fused = fused\\r\\n\\r\\n    def __repr__(self):\\r\\n        return (\\r\\n            f\"{self.__class__.__name__}({self.in_channel}, {self.out_channel}, {self.kernel_size}, \"\\r\\n            f\"upsample={self.upsample}, downsample={self.downsample})\"\\r\\n        )\\r\\n\\r\\n    def forward(self, input, style):\\r\\n        batch, in_channel, height, width = input.shape\\r\\n\\r\\n        if not self.fused:\\r\\n            weight = self.scale * self.weight.squeeze(0)\\r\\n            style = self.modulation(style)\\r\\n\\r\\n            if self.demodulate:\\r\\n                w = weight.unsqueeze(0) * style.view(batch, 1, in_channel, 1, 1)\\r\\n                dcoefs = (w.square().sum((2, 3, 4)) + 1e-8).rsqrt()\\r\\n\\r\\n            input = input * style.reshape(batch, in_channel, 1, 1)\\r\\n\\r\\n            if self.upsample:\\r\\n                weight = weight.transpose(0, 1)\\r\\n                out = conv2d_gradfix.conv_transpose2d(\\r\\n                    input, weight, padding=0, stride=2\\r\\n                )\\r\\n                out = self.blur(out)\\r\\n\\r\\n            elif self.downsample:\\r\\n                input = self.blur(input)\\r\\n                out = conv2d_gradfix.conv2d(input, weight, padding=0, stride=2)\\r\\n\\r\\n            else:\\r\\n                out = conv2d_gradfix.conv2d(input, weight, padding=self.padding)\\r\\n\\r\\n            if self.demodulate:\\r\\n                out = out * dcoefs.view(batch, -1, 1, 1)\\r\\n\\r\\n            return out\\r\\n\\r\\n        style = self.modulation(style).view(batch, 1, in_channel, 1, 1)\\r\\n        weight = self.scale * self.weight * style\\r\\n\\r\\n        if self.demodulate:\\r\\n            demod = torch.rsqrt(weight.pow(2).sum([2, 3, 4]) + 1e-8)\\r\\n            weight = weight * demod.view(batch, self.out_channel, 1, 1, 1)\\r\\n\\r\\n        weight = weight.view(\\r\\n            batch * self.out_channel, in_channel, self.kernel_size, self.kernel_size\\r\\n        )\\r\\n\\r\\n        if self.upsample:\\r\\n            input = input.view(1, batch * in_channel, height, width)\\r\\n            weight = weight.view(\\r\\n                batch, self.out_channel, in_channel, self.kernel_size, self.kernel_size\\r\\n            )\\r\\n            weight = weight.transpose(1, 2).reshape(\\r\\n                batch * in_channel, self.out_channel, self.kernel_size, self.kernel_size\\r\\n            )\\r\\n            out = conv2d_gradfix.conv_transpose2d(\\r\\n                input, weight, padding=0, stride=2, groups=batch\\r\\n            )\\r\\n            _, _, height, width = out.shape\\r\\n            out = out.view(batch, self.out_channel, height, width)\\r\\n            out = self.blur(out)\\r\\n\\r\\n        elif self.downsample:\\r\\n            input = self.blur(input)\\r\\n            _, _, height, width = input.shape\\r\\n            input = input.view(1, batch * in_channel, height, width)\\r\\n            out = conv2d_gradfix.conv2d(\\r\\n                input, weight, padding=0, stride=2, groups=batch\\r\\n            )\\r\\n            _, _, height, width = out.shape\\r\\n            out = out.view(batch, self.out_channel, height, width)\\r\\n\\r\\n        else:\\r\\n            input = input.view(1, batch * in_channel, height, width)\\r\\n            out = conv2d_gradfix.conv2d(\\r\\n                input, weight, padding=self.padding, groups=batch\\r\\n            )\\r\\n            _, _, height, width = out.shape\\r\\n            out = out.view(batch, self.out_channel, height, width)\\r\\n\\r\\n        return out\\r\\n\\r\\n\\r\\nclass NoiseInjection(nn.Module):\\r\\n    def __init__(self):\\r\\n        super().__init__()\\r\\n\\r\\n        self.weight = nn.Parameter(torch.zeros(1))\\r\\n\\r\\n    def forward(self, image, noise=None):\\r\\n        if noise is None:\\r\\n            batch, _, height, width = image.shape\\r\\n            noise = image.new_empty(batch, 1, height, width).normal_()\\r\\n\\r\\n        return image + self.weight * noise\\r\\n\\r\\n\\r\\nclass ConstantInput(nn.Module):\\r\\n    def __init__(self, channel, size=4):\\r\\n        super().__init__()\\r\\n\\r\\n        self.input = nn.Parameter(torch.randn(1, channel, size, size))\\r\\n\\r\\n    def forward(self, input):\\r\\n        batch = input.shape[0]\\r\\n        out = self.input.repeat(batch, 1, 1, 1)\\r\\n\\r\\n        return out\\r\\n\\r\\n\\r\\nclass StyledConv(nn.Module):\\r\\n    def __init__(\\r\\n        self,\\r\\n        in_channel,\\r\\n        out_channel,\\r\\n        kernel_size,\\r\\n        style_dim,\\r\\n        upsample=False,\\r\\n        blur_kernel=[1, 3, 3, 1],\\r\\n        demodulate=True,\\r\\n    ):\\r\\n        super().__init__()\\r\\n\\r\\n        self.conv = ModulatedConv2d(\\r\\n            in_channel,\\r\\n            out_channel,\\r\\n            kernel_size,\\r\\n            style_dim,\\r\\n            upsample=upsample,\\r\\n            blur_kernel=blur_kernel,\\r\\n            demodulate=demodulate,\\r\\n        )\\r\\n\\r\\n        self.noise = NoiseInjection()\\r\\n        # self.bias = nn.Parameter(torch.zeros(1, out_channel, 1, 1))\\r\\n        # self.activate = ScaledLeakyReLU(0.2)\\r\\n        self.activate = FusedLeakyReLU(out_channel)\\r\\n\\r\\n    def forward(self, input, style, noise=None):\\r\\n        out = self.conv(input, style)\\r\\n        out = self.noise(out, noise=noise)\\r\\n        # out = out + self.bias\\r\\n        out = self.activate(out)\\r\\n\\r\\n        return out\\r\\n\\r\\n\\r\\nclass ToRGB(nn.Module):\\r\\n    def __init__(self, in_channel, style_dim, upsample=True, blur_kernel=[1, 3, 3, 1]):\\r\\n        super().__init__()\\r\\n\\r\\n        if upsample:\\r\\n            self.upsample = Upsample(blur_kernel)\\r\\n\\r\\n        self.conv = ModulatedConv2d(in_channel, 3, 1, style_dim, demodulate=False)\\r\\n        self.bias = nn.Parameter(torch.zeros(1, 3, 1, 1))\\r\\n\\r\\n    def forward(self, input, style, skip=None):\\r\\n        out = self.conv(input, style)\\r\\n        out = out + self.bias\\r\\n\\r\\n        if skip is not None:\\r\\n            skip = self.upsample(skip)\\r\\n\\r\\n            out = out + skip\\r\\n\\r\\n        return out\\r\\n\\r\\ndef append_if(condition, var, elem):\\r\\n    if (condition):\\r\\n        var.append(elem)\\r\\n\\r\\nclass Generator(nn.Module):\\r\\n    def __init__(\\r\\n        self,\\r\\n        size,\\r\\n        style_dim,\\r\\n        n_mlp,\\r\\n        channel_multiplier=2,\\r\\n        blur_kernel=[1, 3, 3, 1],\\r\\n        lr_mlp=0.01,\\r\\n    ):\\r\\n        super().__init__()\\r\\n\\r\\n        self.size = size\\r\\n\\r\\n        self.style_dim = style_dim\\r\\n\\r\\n        layers = [PixelNorm()]\\r\\n\\r\\n        for i in range(n_mlp):\\r\\n            layers.append(\\r\\n                EqualLinear(\\r\\n                    style_dim, style_dim, lr_mul=lr_mlp, activation=\"fused_lrelu\"\\r\\n                )\\r\\n            )\\r\\n\\r\\n        self.style = nn.Sequential(*layers)\\r\\n\\r\\n        self.channels = {\\r\\n            4: 512,\\r\\n            8: 512,\\r\\n            16: 512,\\r\\n            32: 512,\\r\\n            64: 256 * channel_multiplier,\\r\\n            128: 128 * channel_multiplier,\\r\\n            256: 64 * channel_multiplier,\\r\\n            512: 32 * channel_multiplier,\\r\\n            1024: 16 * channel_multiplier,\\r\\n        }\\r\\n\\r\\n        self.input = ConstantInput(self.channels[4])\\r\\n        self.conv1 = StyledConv(\\r\\n            self.channels[4], self.channels[4], 3, style_dim, blur_kernel=blur_kernel\\r\\n        )\\r\\n        self.to_rgb1 = ToRGB(self.channels[4], style_dim, upsample=False)\\r\\n\\r\\n        self.log_size = int(math.log(size, 2))\\r\\n        self.num_layers = (self.log_size - 2) * 2 + 1\\r\\n\\r\\n        self.convs = nn.ModuleList()\\r\\n        self.upsamples = nn.ModuleList()\\r\\n        self.to_rgbs = nn.ModuleList()\\r\\n        self.noises = nn.Module()\\r\\n\\r\\n        in_channel = self.channels[4]\\r\\n\\r\\n        for layer_idx in range(self.num_layers):\\r\\n            res = (layer_idx + 5) // 2\\r\\n            shape = [1, 1, 2 ** res, 2 ** res]\\r\\n            self.noises.register_buffer(f\"noise_{layer_idx}\", torch.randn(*shape))\\r\\n\\r\\n        for i in range(3, self.log_size + 1):\\r\\n            out_channel = self.channels[2 ** i]\\r\\n\\r\\n            self.convs.append(\\r\\n                StyledConv(\\r\\n                    in_channel,\\r\\n                    out_channel,\\r\\n                    3,\\r\\n                    style_dim,\\r\\n                    upsample=True,\\r\\n                    blur_kernel=blur_kernel,\\r\\n                )\\r\\n            )\\r\\n\\r\\n            self.convs.append(\\r\\n                StyledConv(\\r\\n                    out_channel, out_channel, 3, style_dim, blur_kernel=blur_kernel\\r\\n                )\\r\\n            )\\r\\n\\r\\n            self.to_rgbs.append(ToRGB(out_channel, style_dim))\\r\\n\\r\\n            in_channel = out_channel\\r\\n\\r\\n        self.n_latent = self.log_size * 2 - 2\\r\\n\\r\\n    def make_noise(self):\\r\\n        device = self.input.input.device\\r\\n\\r\\n        noises = [torch.randn(1, 1, 2 ** 2, 2 ** 2, device=device)]\\r\\n\\r\\n        for i in range(3, self.log_size + 1):\\r\\n            for _ in range(2):\\r\\n                noises.append(torch.randn(1, 1, 2 ** i, 2 ** i, device=device))\\r\\n\\r\\n        return noises\\r\\n\\r\\n    def mean_latent(self, n_latent):\\r\\n        latent_in = torch.randn(\\r\\n            n_latent, self.style_dim, device=self.input.input.device\\r\\n        )\\r\\n        latent = self.style(latent_in).mean(0, keepdim=True)\\r\\n\\r\\n        return latent\\r\\n\\r\\n    def get_latent(self, input):\\r\\n        return self.style(input)\\r\\n\\r\\n    def forward(\\r\\n        self,\\r\\n        styles,\\r\\n        return_latents=False,\\r\\n        inject_index=None,\\r\\n        truncation=1,\\r\\n        truncation_latent=None,\\r\\n        input_is_latent=False,\\r\\n        noise=None,\\r\\n        randomize_noise=True,\\r\\n        return_features=False,\\r\\n    ):\\r\\n        if not input_is_latent:\\r\\n            styles = [self.style(s) for s in styles]\\r\\n\\r\\n        if noise is None:\\r\\n            if randomize_noise:\\r\\n                noise = [None] * self.num_layers\\r\\n            else:\\r\\n                noise = [\\r\\n                    getattr(self.noises, f\"noise_{i}\") for i in range(self.num_layers)\\r\\n                ]\\r\\n\\r\\n        if truncation < 1:\\r\\n            style_t = []\\r\\n\\r\\n            for style in styles:\\r\\n                style_t.append(\\r\\n                    truncation_latent + truncation * (style - truncation_latent)\\r\\n                )\\r\\n\\r\\n            styles = style_t\\r\\n\\r\\n        if len(styles) < 2:\\r\\n            inject_index = self.n_latent\\r\\n\\r\\n            if styles[0].ndim < 3:\\r\\n                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\\r\\n\\r\\n            else:\\r\\n                latent = styles[0]\\r\\n\\r\\n        else:\\r\\n            if inject_index is None:\\r\\n                inject_index = random.randint(1, self.n_latent - 1)\\r\\n\\r\\n            latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\\r\\n            latent2 = styles[1].unsqueeze(1).repeat(1, self.n_latent - inject_index, 1)\\r\\n\\r\\n            latent = torch.cat([latent, latent2], 1)\\r\\n\\r\\n        features = []\\r\\n        out = self.input(latent)\\r\\n        append_if(return_features, features, out)\\r\\n        out = self.conv1(out, latent[:, 0], noise=noise[0])\\r\\n        append_if(return_features, features, out)\\r\\n\\r\\n        skip = self.to_rgb1(out, latent[:, 1])\\r\\n\\r\\n        i = 1\\r\\n        for conv1, conv2, noise1, noise2, to_rgb in zip(\\r\\n            self.convs[::2], self.convs[1::2], noise[1::2], noise[2::2], self.to_rgbs\\r\\n        ):\\r\\n            out = conv1(out, latent[:, i], noise=noise1)\\r\\n            append_if(return_features, features, out)\\r\\n            out = conv2(out, latent[:, i + 1], noise=noise2)\\r\\n            append_if(return_features, features, out)\\r\\n            skip = to_rgb(out, latent[:, i + 2], skip)\\r\\n\\r\\n            i += 2\\r\\n\\r\\n        image = skip\\r\\n\\r\\n        if return_latents:\\r\\n            return image, latent, features\\r\\n\\r\\n        else:\\r\\n            return image, None, features\\r\\n\\r\\n\\r\\nclass ConvLayer(nn.Sequential):\\r\\n    def __init__(\\r\\n        self,\\r\\n        in_channel,\\r\\n        out_channel,\\r\\n        kernel_size,\\r\\n        downsample=False,\\r\\n        blur_kernel=[1, 3, 3, 1],\\r\\n        bias=True,\\r\\n        activate=True,\\r\\n    ):\\r\\n        layers = []\\r\\n\\r\\n        if downsample:\\r\\n            factor = 2\\r\\n            p = (len(blur_kernel) - factor) + (kernel_size - 1)\\r\\n            pad0 = (p + 1) // 2\\r\\n            pad1 = p // 2\\r\\n\\r\\n            layers.append(Blur(blur_kernel, pad=(pad0, pad1)))\\r\\n\\r\\n            stride = 2\\r\\n            self.padding = 0\\r\\n\\r\\n        else:\\r\\n            stride = 1\\r\\n            self.padding = kernel_size // 2\\r\\n\\r\\n        layers.append(\\r\\n            EqualConv2d(\\r\\n                in_channel,\\r\\n                out_channel,\\r\\n                kernel_size,\\r\\n                padding=self.padding,\\r\\n                stride=stride,\\r\\n                bias=bias and not activate,\\r\\n            )\\r\\n        )\\r\\n\\r\\n        if activate:\\r\\n            layers.append(FusedLeakyReLU(out_channel, bias=bias))\\r\\n\\r\\n        super().__init__(*layers)\\r\\n\\r\\n\\r\\nclass ResBlock(nn.Module):\\r\\n    def __init__(self, in_channel, out_channel, blur_kernel=[1, 3, 3, 1]):\\r\\n        super().__init__()\\r\\n\\r\\n        self.conv1 = ConvLayer(in_channel, in_channel, 3)\\r\\n        self.conv2 = ConvLayer(in_channel, out_channel, 3, downsample=True)\\r\\n\\r\\n        self.skip = ConvLayer(\\r\\n            in_channel, out_channel, 1, downsample=True, activate=False, bias=False\\r\\n        )\\r\\n\\r\\n    def forward(self, input):\\r\\n        out = self.conv1(input)\\r\\n        out = self.conv2(out)\\r\\n\\r\\n        skip = self.skip(input)\\r\\n        out = (out + skip) / math.sqrt(2)\\r\\n\\r\\n        return out\\r\\n\\r\\n\\r\\nclass Discriminator(nn.Module):\\r\\n    def __init__(self, size, channel_multiplier=2, blur_kernel=[1, 3, 3, 1]):\\r\\n        super().__init__()\\r\\n\\r\\n        channels = {\\r\\n            4: 512,\\r\\n            8: 512,\\r\\n            16: 512,\\r\\n            32: 512,\\r\\n            64: 256 * channel_multiplier,\\r\\n            128: 128 * channel_multiplier,\\r\\n            256: 64 * channel_multiplier,\\r\\n            512: 32 * channel_multiplier,\\r\\n            1024: 16 * channel_multiplier,\\r\\n        }\\r\\n\\r\\n        convs = [ConvLayer(3, channels[size], 1)]\\r\\n\\r\\n        log_size = int(math.log(size, 2))\\r\\n\\r\\n        in_channel = channels[size]\\r\\n\\r\\n        for i in range(log_size, 2, -1):\\r\\n            out_channel = channels[2 ** (i - 1)]\\r\\n\\r\\n            convs.append(ResBlock(in_channel, out_channel, blur_kernel))\\r\\n\\r\\n            in_channel = out_channel\\r\\n\\r\\n        self.convs = nn.Sequential(*convs)\\r\\n\\r\\n        self.stddev_group = 4\\r\\n        self.stddev_feat = 1\\r\\n\\r\\n        self.final_conv = ConvLayer(in_channel + 1, channels[4], 3)\\r\\n        self.final_linear = nn.Sequential(\\r\\n            EqualLinear(channels[4] * 4 * 4, channels[4], activation=\"fused_lrelu\"),\\r\\n            EqualLinear(channels[4], 1),\\r\\n        )\\r\\n\\r\\n    def forward(self, input):\\r\\n        out = self.convs(input)\\r\\n\\r\\n        batch, channel, height, width = out.shape\\r\\n        group = min(batch, self.stddev_group)\\r\\n        stddev = out.view(\\r\\n            group, -1, self.stddev_feat, channel // self.stddev_feat, height, width\\r\\n        )\\r\\n        stddev = torch.sqrt(stddev.var(0, unbiased=False) + 1e-8)\\r\\n        stddev = stddev.mean([2, 3, 4], keepdims=True).squeeze(2)\\r\\n        stddev = stddev.repeat(group, 1, height, width)\\r\\n        out = torch.cat([out, stddev], 1)\\r\\n\\r\\n        out = self.final_conv(out)\\r\\n\\r\\n        out = out.view(batch, -1)\\r\\n        out = self.final_linear(out)\\r\\n\\r\\n        return out\\r\\n\\r\\n', metadata={'file_path': 'stylegan2.py', 'file_name': 'stylegan2.py', 'file_type': '.py'}),\n",
       " Document(page_content='from .fused_act import FusedLeakyReLU, fused_leaky_relu\\r\\nfrom .upfirdn2d import upfirdn2d\\r\\n', metadata={'file_path': 'op\\\\__init__.py', 'file_name': '__init__.py', 'file_type': '.py'}),\n",
       " Document(page_content='*.png', metadata={'file_path': 'sample\\\\.gitignore', 'file_name': '.gitignore', 'file_type': ''}),\n",
       " Document(page_content='from collections import abc\\r\\nimport torch\\r\\nfrom torch.nn import functional as F\\r\\n\\r\\n\\r\\ndef upfirdn2d(input, kernel, up=1, down=1, pad=(0, 0)):\\r\\n    if not isinstance(up, abc.Iterable):\\r\\n        up = (up, up)\\r\\n\\r\\n    if not isinstance(down, abc.Iterable):\\r\\n        down = (down, down)\\r\\n\\r\\n    if len(pad) == 2:\\r\\n        pad = (pad[0], pad[1], pad[0], pad[1])\\r\\n\\r\\n    out = upfirdn2d_native(input, kernel, *up, *down, *pad)\\r\\n\\r\\n    return out\\r\\n\\r\\n\\r\\ndef upfirdn2d_native(\\r\\n    input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1\\r\\n):\\r\\n    _, channel, in_h, in_w = input.shape\\r\\n    input = input.reshape(-1, in_h, in_w, 1)\\r\\n\\r\\n    _, in_h, in_w, minor = input.shape\\r\\n    kernel_h, kernel_w = kernel.shape\\r\\n\\r\\n    out = input.view(-1, in_h, 1, in_w, 1, minor)\\r\\n    out = F.pad(out, [0, 0, 0, up_x - 1, 0, 0, 0, up_y - 1])\\r\\n    out = out.view(-1, in_h * up_y, in_w * up_x, minor)\\r\\n\\r\\n    out = F.pad(\\r\\n        out, [0, 0, max(pad_x0, 0), max(pad_x1, 0), max(pad_y0, 0), max(pad_y1, 0)]\\r\\n    )\\r\\n    out = out[\\r\\n        :,\\r\\n        max(-pad_y0, 0) : out.shape[1] - max(-pad_y1, 0),\\r\\n        max(-pad_x0, 0) : out.shape[2] - max(-pad_x1, 0),\\r\\n        :,\\r\\n    ]\\r\\n\\r\\n    out = out.permute(0, 3, 1, 2)\\r\\n    out = out.reshape(\\r\\n        [-1, 1, in_h * up_y + pad_y0 + pad_y1, in_w * up_x + pad_x0 + pad_x1]\\r\\n    )\\r\\n    w = torch.flip(kernel, [0, 1]).view(1, 1, kernel_h, kernel_w)\\r\\n    out = F.conv2d(out, w)\\r\\n    out = out.reshape(\\r\\n        -1,\\r\\n        minor,\\r\\n        in_h * up_y + pad_y0 + pad_y1 - kernel_h + 1,\\r\\n        in_w * up_x + pad_x0 + pad_x1 - kernel_w + 1,\\r\\n    )\\r\\n    out = out.permute(0, 2, 3, 1)\\r\\n    out = out[:, ::down_y, ::down_x, :]\\r\\n\\r\\n    out_h = (in_h * up_y + pad_y0 + pad_y1 - kernel_h + down_y) // down_y\\r\\n    out_w = (in_w * up_x + pad_x0 + pad_x1 - kernel_w + down_x) // down_x\\r\\n\\r\\n    return out.view(-1, channel, out_h, out_w)\\r\\n', metadata={'file_path': 'op\\\\upfirdn2d.py', 'file_name': 'upfirdn2d.py', 'file_type': '.py'}),\n",
       " Document(page_content=\"# Byte-compiled / optimized / DLL files\\r\\n__pycache__/\\r\\n*.py[cod]\\r\\n*$py.class\\r\\n\\r\\n# C extensions\\r\\n*.so\\r\\n\\r\\n# Distribution / packaging\\r\\n.Python\\r\\nbuild/\\r\\ndevelop-eggs/\\r\\ndist/\\r\\ndownloads/\\r\\neggs/\\r\\n.eggs/\\r\\nlib/\\r\\nlib64/\\r\\nparts/\\r\\nsdist/\\r\\nvar/\\r\\nwheels/\\r\\npip-wheel-metadata/\\r\\nshare/python-wheels/\\r\\n*.egg-info/\\r\\n.installed.cfg\\r\\n*.egg\\r\\nMANIFEST\\r\\n\\r\\n# PyInstaller\\r\\n#  Usually these files are written by a python script from a template\\r\\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\\r\\n*.manifest\\r\\n*.spec\\r\\n\\r\\n# Installer logs\\r\\npip-log.txt\\r\\npip-delete-this-directory.txt\\r\\n\\r\\n# Unit test / coverage reports\\r\\nhtmlcov/\\r\\n.tox/\\r\\n.nox/\\r\\n.coverage\\r\\n.coverage.*\\r\\n.cache\\r\\nnosetests.xml\\r\\ncoverage.xml\\r\\n*.cover\\r\\n*.py,cover\\r\\n.hypothesis/\\r\\n.pytest_cache/\\r\\n\\r\\n# Translations\\r\\n*.mo\\r\\n*.pot\\r\\n\\r\\n# Django stuff:\\r\\n*.log\\r\\nlocal_settings.py\\r\\ndb.sqlite3\\r\\ndb.sqlite3-journal\\r\\n\\r\\n# Flask stuff:\\r\\ninstance/\\r\\n.webassets-cache\\r\\n\\r\\n# Scrapy stuff:\\r\\n.scrapy\\r\\n\\r\\n# Sphinx documentation\\r\\ndocs/_build/\\r\\n\\r\\n# PyBuilder\\r\\ntarget/\\r\\n\\r\\n# Jupyter Notebook\\r\\n.ipynb_checkpoints\\r\\n\\r\\n# IPython\\r\\nprofile_default/\\r\\nipython_config.py\\r\\n\\r\\n# pyenv\\r\\n.python-version\\r\\n\\r\\n# pipenv\\r\\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\\r\\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\\r\\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\\r\\n#   install all needed dependencies.\\r\\n#Pipfile.lock\\r\\n\\r\\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\\r\\n__pypackages__/\\r\\n\\r\\n# Celery stuff\\r\\ncelerybeat-schedule\\r\\ncelerybeat.pid\\r\\n\\r\\n# SageMath parsed files\\r\\n*.sage.py\\r\\n\\r\\n# Environments\\r\\n.env\\r\\n.venv\\r\\nenv/\\r\\nvenv/\\r\\nENV/\\r\\nenv.bak/\\r\\nvenv.bak/\\r\\n\\r\\n# Spyder project settings\\r\\n.spyderproject\\r\\n.spyproject\\r\\n\\r\\n# Rope project settings\\r\\n.ropeproject\\r\\n\\r\\n# mkdocs documentation\\r\\n/site\\r\\n\\r\\n# mypy\\r\\n.mypy_cache/\\r\\n.dmypy.json\\r\\ndmypy.json\\r\\n\\r\\n# Pyre type checker\\r\\n.pyre/\\r\\n\", metadata={'file_path': '.gitignore', 'file_name': '.gitignore', 'file_type': ''}),\n",
       " Document(page_content='MIT License\\r\\n\\r\\nCopyright (c) 2023 Mr. Zhang\\r\\n\\r\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\r\\nof this software and associated documentation files (the \"Software\"), to deal\\r\\nin the Software without restriction, including without limitation the rights\\r\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\r\\ncopies of the Software, and to permit persons to whom the Software is\\r\\nfurnished to do so, subject to the following conditions:\\r\\n\\r\\nThe above copyright notice and this permission notice shall be included in all\\r\\ncopies or substantial portions of the Software.\\r\\n\\r\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\r\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\r\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\r\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\r\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\r\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\r\\nSOFTWARE.\\r\\n', metadata={'file_path': 'LICENSE', 'file_name': 'LICENSE', 'file_type': ''}),\n",
       " Document(page_content='<img src=\"./draggan.png\" width=\"750\" alt=\"Architecture of DragGAN\"/>\\r\\n\\r\\n# DragGAN\\r\\nImplementation of [DragGAN: Interactive Point-based Manipulation on the Generative Image Manifold](https://arxiv.org/abs/2305.10973).\\r\\n\\r\\n```shell\\r\\n# gui\\r\\npip install dearpygui\\r\\n# run demo\\r\\npython gui.py\\r\\n```\\r\\n\\r\\n<img src=\"./UI.png\" width=\"600\" alt=\"Demo UI\"/>\\r\\n\\r\\n<img src=\"./sample/start.png\" width=\"200\" alt=\"Demo Start\"/><img src=\"./sample/end.png\" width=\"200\" alt=\"Demo End\"/><img src=\"./sample/multi-point.png\" width=\"200\" alt=\"Multi Point Control\"/>\\r\\n\\r\\n# TODO\\r\\n- [x] GUI\\r\\n- [x] drag it\\r\\n- [ ] load real image\\r\\n- [ ] mask\\r\\n\\r\\n# StyleGAN2 Pre-Trained Model\\r\\nRosinality\\'s pre-trained model(256px) on FFHQ 550k iterations \\\\[[Link](https://drive.google.com/open?id=1PQutd-JboOCOZqmd95XWxWrO8gGEvRcO)\\\\].\\r\\n\\r\\n# References\\r\\n- https://github.com/rosinality/stylegan2-pytorch\\r\\n', metadata={'file_path': 'README.md', 'file_name': 'README.md', 'file_type': '.md'})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline for get_top_repos() -> pull_code_from_repo()\n",
    "github_data = loader.pipeline_fetch_and_load(\n",
    "    n_repos=10, last_n_days=30, language=\"python\", delete=True\n",
    ")\n",
    "\n",
    "# show example document list from the DragGAN repo\n",
    "github_data['https://github.com/JiauZhang/DragGAN']['docs']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
